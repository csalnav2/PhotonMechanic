{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit --quiet"
      ],
      "metadata": {
        "id": "PMwmE-AL4_H7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ce7e59-8c6d-41de-cbd7-4c453e608ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet # Install the pyngrok module\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "K4FDMuu6J2nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Li4tEphLLWO",
        "outputId": "aabd7996-72a2-4376-c5f7-d3d4ba0b82a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "..............................."
      ],
      "metadata": {
        "id": "F_sb-ywMb9ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 $(lsof -t -i:8000) 2>/dev/null || echo \"No existing process\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls_NhDtSLZvZ",
        "outputId": "6eff212c-599c-48d6-a128-4549ee928ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No existing process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet flask transformers torch networkx matplotlib scikit-learn safetensors pyvis soundfile psutil GPUtil\n",
        "# no dynamic quant error on GPU for int8 => fallback to CPU"
      ],
      "metadata": {
        "id": "4lA0OIYIUCWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44a1dfa-f18f-4e09-999a-e8ff1578c15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f ngrok"
      ],
      "metadata": {
        "id": "xH8QxPx04R5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import threading\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx\n",
        "from flask import Flask, request, jsonify, Response\n",
        "from collections import deque\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "import traceback\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------- USER CONFIG ----------------\n",
        "SCENARIO = \"ngrok\"  # \"domain\", \"ngrok\", or \"cloudflared\"\n",
        "HUGGINGFACE_TOKEN = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"        # <-- Replace\n",
        "NGROK_AUTH_TOKEN = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # <-- Replace\n",
        "PORT = 8000\n",
        "\n",
        "# ---------------- LOG BUFFER ----------------\n",
        "LOG_BUFFER = deque(maxlen=100)\n",
        "def log_msg(msg):\n",
        "    LOG_BUFFER.append(msg)\n",
        "    print(msg, flush=True)\n",
        "\n",
        "# -------------- KILL ANYTHING ON PORT=8000 --------------\n",
        "def kill_port_8000():\n",
        "    try:\n",
        "        os.system(\"pkill -f ngrok 2>/dev/null || echo 'No old ngrok.'\")\n",
        "        os.system(\"pkill -f cloudflared 2>/dev/null || echo 'No old cloudflared.'\")\n",
        "        os.system(\"kill -9 $(lsof -t -i:8000) 2>/dev/null || echo 'No old process on 8000.'\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "log_msg(f\"ğŸ”´ Killing anything on port={PORT} + old ngrok/cloudflared.\")\n",
        "kill_port_8000()\n",
        "\n",
        "# --------------- INSTALL DEPS ---------------\n",
        "log_msg(\"ğŸ“¦ Installing BFS dependencies quietly...\")\n",
        "deps = [\n",
        "    \"flask\", \"transformers\", \"torch\", \"accelerate\", \"peft\", \"bitsandbytes\", \"pyngrok\",\n",
        "    \"flask_cloudflared\", \"networkx\", \"matplotlib\", \"pyvis\", \"psutil\", \"requests\",\n",
        "    \"scipy\", \"GPUtil\", \"plotly\", \"huggingface_hub\", \"pandas\"\n",
        "]\n",
        "os.system(\"pip install --quiet \" + \" \".join(deps))\n",
        "\n",
        "# --------------- HUGGING FACE LOGIN ---------------\n",
        "log_msg(\"ğŸ”‘ Logging into Hugging Face...\")\n",
        "try:\n",
        "    from huggingface_hub import login\n",
        "    login(HUGGINGFACE_TOKEN)\n",
        "except Exception as e:\n",
        "    log_msg(f\"â— Could not login => {e}\")\n",
        "\n",
        "# --------------- FLASK APP ---------------\n",
        "app = Flask(__name__, static_folder=\"static\")\n",
        "if not os.path.exists(\"static\"):\n",
        "    os.makedirs(\"static\")\n",
        "\n",
        "LOG_DIR = \"model_logs\"\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "# --------------- BFS GLOBALS ---------------\n",
        "RL_CHOICE = \"No RL\"\n",
        "HYP_GRAPH = None\n",
        "COMP_GRAPH = None\n",
        "OLD_HYP_GRAPH = None\n",
        "OLD_COMP_GRAPH = None\n",
        "MODEL_CACHE = {}\n",
        "Q_TABLE = {s: [random.uniform(-1,1) for _ in range(4)] for s in range(10)}\n",
        "\n",
        "GPU_MEMORY_SCALE = {\n",
        "    \"T4\": 1.0,\n",
        "    \"A100\": 0.5,\n",
        "    \"V100\": 0.7,\n",
        "    \"TPU\": 0.4\n",
        "}\n",
        "\n",
        "BFS_CANCEL = False\n",
        "BFS_RUNNING = False\n",
        "BFS_FAIL = False\n",
        "BFS_THREAD = None\n",
        "\n",
        "CURRENT_MODEL_LABEL = None\n",
        "CURRENT_QUANT = None\n",
        "CURRENT_REASONING = None\n",
        "CURRENT_RL = None\n",
        "\n",
        "FLOP_RANDOM_FACTORS = {}\n",
        "\n",
        "# --------------- MODEL LIST ---------------\n",
        "AVAILABLE_MODELS = {\n",
        "    \"BERT (base)\": \"bert-base-uncased\",\n",
        "    \"GPT-2\": \"gpt2\",\n",
        "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
        "    \"Falcon-7B\": \"tiiuae/falcon-7b-instruct\",\n",
        "    \"Flan-UL2 (20B)\": \"google/flan-ul2\",\n",
        "    \"Qwen-30B\": \"Qwen/Qwen-30B\"\n",
        "}\n",
        "MODEL_SIZE_INFO = {\n",
        "    \"BERT (base)\":110e6,\n",
        "    \"GPT-2\":124e6,\n",
        "    \"DistilBERT\":66e6,\n",
        "    \"Falcon-7B\":7e9,\n",
        "    \"Flan-UL2 (20B)\":20e9,\n",
        "    \"Qwen-30B\":30e9\n",
        "}\n",
        "QUANT_METHODS = [\n",
        "    \"None (FP32)\",\"FP16\",\"INT8\",\"4-bit (bitsandbytes)\",\"8-bit (bitsandbytes)\"\n",
        "]\n",
        "REASONING_METHODS = [\n",
        "    \"No reasoning\",\"Monte Carlo Tree Search (MCTS)\",\n",
        "    \"Chain of Thought (CoT)\",\"Meta-CoT\",\"Neural-Symbolic\"\n",
        "]\n",
        "RL_ALGORITHMS = [\n",
        "    \"No RL\",\"Q-Learning\",\"Deep Q-Network (DQN)\",\n",
        "    \"Policy Gradient\",\"Actor-Critic\",\"PPO\"\n",
        "]\n",
        "ATTN_COMPLEXITY_OPTS = [\n",
        "    \"Quadratic (n^2)\",\"Logarithmic (n log n)\",\"Linear (n)\"\n",
        "]\n",
        "\n",
        "FIXED_LIN_COMP = \"Linear (n)\"\n",
        "FIXED_DEF_COMP = \"Linear (n)\"\n",
        "\n",
        "# =============== BFS UTILS ===============\n",
        "def keep_largest_connected_component(nx_graph):\n",
        "    if nx_graph.number_of_nodes()==0:\n",
        "        return nx_graph\n",
        "    ug= nx_graph.to_undirected()\n",
        "    comps= list(nx.connected_components(ug))\n",
        "    if not comps:\n",
        "        return nx_graph\n",
        "    largest= max(comps, key=len)\n",
        "    return nx_graph.subgraph(largest).copy()\n",
        "\n",
        "def compute_bfs_levels(nx_graph):\n",
        "    if nx_graph.number_of_nodes()==0:\n",
        "        return\n",
        "    for n in nx_graph.nodes:\n",
        "        nx_graph.nodes[n][\"BFS_distance\"]=0\n",
        "    root_candidates= [n for n in nx_graph.nodes if \"input\" in n.lower()]\n",
        "    if not root_candidates:\n",
        "        in_degs= dict(nx_graph.in_degree())\n",
        "        zero_in= [nd for nd,deg in in_degs.items() if deg==0]\n",
        "        if zero_in:\n",
        "            root_candidates= zero_in\n",
        "        if not root_candidates:\n",
        "            root_candidates= [list(nx_graph.nodes)[0]]\n",
        "    visited= set()\n",
        "    queue= []\n",
        "    for r in root_candidates:\n",
        "        queue.append((r,0))\n",
        "        nx_graph.nodes[r][\"BFS_distance\"]=0\n",
        "        visited.add(r)\n",
        "    while queue:\n",
        "        if BFS_CANCEL:\n",
        "            log_msg(\"ğŸ”´ BFS CANCEL => early exit BFS levels.\")\n",
        "            return\n",
        "        current, dist= queue.pop(0)\n",
        "        for neighbor in nx_graph.neighbors(current):\n",
        "            if neighbor not in visited:\n",
        "                visited.add(neighbor)\n",
        "                nx_graph.nodes[neighbor][\"BFS_distance\"]= dist+1\n",
        "                queue.append((neighbor, dist+1))\n",
        "\n",
        "def stable_flop_factor(node_str):\n",
        "    if node_str not in FLOP_RANDOM_FACTORS:\n",
        "        FLOP_RANDOM_FACTORS[node_str]= random.uniform(0.5,2.0)\n",
        "    return FLOP_RANDOM_FACTORS[node_str]\n",
        "\n",
        "def interpret_time_cost(node_str, attn_comp, lin_comp, def_comp, seq_len):\n",
        "    low= node_str.lower()\n",
        "    if \"attention\" in low or \"matmul\" in low:\n",
        "        chosen= attn_comp\n",
        "    elif \"linear\" in low:\n",
        "        chosen= lin_comp\n",
        "    else:\n",
        "        chosen= def_comp\n",
        "    import math\n",
        "    if chosen.startswith(\"Quadratic\"):\n",
        "        base= seq_len**2\n",
        "    elif chosen.startswith(\"Logarithmic\"):\n",
        "        base= int(seq_len*math.log2(seq_len+1))\n",
        "    else:\n",
        "        base= seq_len\n",
        "    factor= stable_flop_factor(node_str)\n",
        "    return base* factor\n",
        "\n",
        "def compute_node_flops(node_str, attn_comp, lin_comp, def_comp, seq_len):\n",
        "    base_t= interpret_time_cost(node_str, attn_comp, lin_comp, def_comp, seq_len)\n",
        "    return base_t*5.0\n",
        "\n",
        "# ------------- HOOK-BASED (ONLY) -------------\n",
        "def hook_based_extract(model, tokenizer):\n",
        "    import torch\n",
        "    import networkx as nx\n",
        "    g= nx.DiGraph()\n",
        "    call_seq=[]\n",
        "    def make_hook(mod_name):\n",
        "        def fwd_hook(module,inp,outp):\n",
        "            if BFS_CANCEL:\n",
        "                return\n",
        "            call_seq.append(mod_name)\n",
        "        return fwd_hook\n",
        "    for nm, subm in model.named_modules():\n",
        "        subm.register_forward_hook(make_hook(nm))\n",
        "    device= next(model.parameters()).device\n",
        "    enc= tokenizer(\"Hello hooking approach!\", return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        _= model(**enc)\n",
        "    # build hooking graph\n",
        "    for nm in set(call_seq):\n",
        "        g.add_node(\"COMP_hook_\"+ nm, BFS_distance=0,mem=0,timecost=0,target_mem=0,target_time=0,flops=0)\n",
        "    for i in range(len(call_seq)-1):\n",
        "        parent= \"COMP_hook_\"+ call_seq[i]\n",
        "        child=  \"COMP_hook_\"+ call_seq[i+1]\n",
        "        g.add_edge(parent, child)\n",
        "    node_count= g.number_of_nodes()\n",
        "    edge_count= g.number_of_edges()\n",
        "    log_msg(f\"Hook-based => discovered {node_count} nodes, {edge_count} edges from forward pass.\")\n",
        "    return g\n",
        "\n",
        "def fallback_comp_graph_for_model(model):\n",
        "    import networkx as nx\n",
        "    g= nx.DiGraph()\n",
        "    for name,_ in model.named_modules():\n",
        "        node_name= \"COMP_\"+ name\n",
        "        g.add_node(node_name,BFS_distance=0,mem=0,timecost=0,target_mem=0,target_time=0,flops=0)\n",
        "        if \".\" in name:\n",
        "            parent= \"COMP_\"+ name.rsplit(\".\",1)[0]\n",
        "            child= \"COMP_\"+ name\n",
        "            g.add_edge(parent, child)\n",
        "    return g\n",
        "\n",
        "def extract_comp_graph(model, tokenizer):\n",
        "    # Just hooking => if hooking <2 nodes => fallback\n",
        "    try:\n",
        "        g_hook= hook_based_extract(model, tokenizer)\n",
        "        if g_hook.number_of_nodes()>=2:\n",
        "            log_msg(\"âœ… Hook-based => returning hooking graph.\")\n",
        "            return g_hook\n",
        "        else:\n",
        "            log_msg(\"âš ï¸ Hook-based => <2 nodes => fallback.\")\n",
        "    except Exception as e:\n",
        "        log_msg(f\"Hook => fail => {e}\")\n",
        "    log_msg(\"âš ï¸ named_modules fallback => minimal graph.\")\n",
        "    return fallback_comp_graph_for_model(model)\n",
        "\n",
        "def extract_hypergraph(model):\n",
        "    import networkx as nx\n",
        "    g= nx.DiGraph()\n",
        "    for name,_ in model.named_modules():\n",
        "        node_name= \"HYP_\"+ name\n",
        "        g.add_node(node_name)\n",
        "        if \".\" in name:\n",
        "            parent= \"HYP_\"+ name.rsplit(\".\",1)[0]\n",
        "            child=  \"HYP_\"+ name\n",
        "            g.add_edge(parent, child)\n",
        "    return g\n",
        "\n",
        "# --------------- BFS => color => concurrency ---------------\n",
        "def mark_graph_changes(new_graph, old_graph):\n",
        "    if not old_graph:\n",
        "        for n in new_graph.nodes:\n",
        "            new_graph.nodes[n][\"color\"]=\"green\"\n",
        "        for e in new_graph.edges:\n",
        "            new_graph[e[0]][e[1]][\"color\"]=\"green\"\n",
        "        return\n",
        "    old_nodes= old_graph.nodes(data=True)\n",
        "    old_edges= set(old_graph.edges())\n",
        "    for n,d in new_graph.nodes(data=True):\n",
        "        if n not in old_graph:\n",
        "            d[\"color\"]=\"green\"\n",
        "        else:\n",
        "            old_data= old_nodes[n]\n",
        "            changed=False\n",
        "            fields=[\"BFS_distance\",\"mem\",\"timecost\",\"target_mem\",\"target_time\",\"flops\"]\n",
        "            for f in fields:\n",
        "                old_val= old_data.get(f,None)\n",
        "                new_val= d.get(f,None)\n",
        "                if old_val!= new_val:\n",
        "                    changed=True\n",
        "                    break\n",
        "            if changed:\n",
        "                d[\"color\"]=\"red\"\n",
        "            else:\n",
        "                d[\"color\"]=\"blue\"\n",
        "    new_edges= set(new_graph.edges())\n",
        "    for e in new_edges:\n",
        "        if e not in old_edges and (e[1], e[0]) not in old_edges:\n",
        "            new_graph[e[0]][e[1]][\"color\"]=\"green\"\n",
        "        else:\n",
        "            new_graph[e[0]][e[1]][\"color\"]=\"gray\"\n",
        "\n",
        "def update_q_table(state,action,reward):\n",
        "    old_val= Q_TABLE[state][action]\n",
        "    new_val= old_val+0.1*(reward- old_val)\n",
        "    Q_TABLE[state][action]= new_val\n",
        "\n",
        "# --------------- auto_force_quant + load_model ---------------\n",
        "def auto_force_quant(model_label, user_choice):\n",
        "    size= MODEL_SIZE_INFO.get(model_label,0)\n",
        "    if size>=1e9 and size<10e9:\n",
        "        return \"8-bit (bitsandbytes)\"\n",
        "    elif size>=10e9:\n",
        "        return \"4-bit (bitsandbytes)\"\n",
        "    else:\n",
        "        return user_choice\n",
        "\n",
        "def load_model(hf_id, model_label, quant=\"None (FP32)\"):\n",
        "    import torch.nn as nn\n",
        "    import torch\n",
        "    forced_quant= auto_force_quant(model_label, quant)\n",
        "    if forced_quant!= quant:\n",
        "        log_msg(f\"âš ï¸ Overriding user quant={quant} => forced to {forced_quant}.\")\n",
        "        quant= forced_quant\n",
        "    device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if quant==\"INT8\" and device==\"cuda\":\n",
        "        device=\"cpu\"\n",
        "    cache_key= f\"{hf_id}_{quant}_{device}\"\n",
        "    if cache_key in MODEL_CACHE:\n",
        "        return MODEL_CACHE[cache_key][\"model\"], MODEL_CACHE[cache_key][\"tokenizer\"]\n",
        "\n",
        "    from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "    def try_causallm():\n",
        "        return AutoModelForCausalLM.from_pretrained(hf_id)\n",
        "    def try_auto():\n",
        "        return AutoModel.from_pretrained(hf_id)\n",
        "\n",
        "    try:\n",
        "        model= try_causallm()\n",
        "    except:\n",
        "        model= try_auto()\n",
        "\n",
        "    tokenizer= AutoTokenizer.from_pretrained(hf_id)\n",
        "    if not tokenizer.pad_token_id:\n",
        "        tokenizer.pad_token_id= tokenizer.eos_token_id or 0\n",
        "\n",
        "    if quant==\"FP16\":\n",
        "        model= model.half()\n",
        "    elif quant==\"INT8\":\n",
        "        model= torch.quantization.quantize_dynamic(model,{nn.Linear},dtype=torch.qint8)\n",
        "    elif quant in [\"4-bit (bitsandbytes)\",\"8-bit (bitsandbytes)\"]:\n",
        "        from transformers import BitsAndBytesConfig,AutoModelForCausalLM\n",
        "        if \"4-bit\" in quant:\n",
        "            bnb_config= BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16)\n",
        "        else:\n",
        "            bnb_config= BitsAndBytesConfig(load_in_8bit=True)\n",
        "        model= AutoModelForCausalLM.from_pretrained(hf_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "        model.eval()\n",
        "        MODEL_CACHE[cache_key]={\"model\":model,\"tokenizer\":tokenizer}\n",
        "        return model, tokenizer\n",
        "\n",
        "    model.eval()\n",
        "    if quant not in [\"INT8\",\"4-bit (bitsandbytes)\",\"8-bit (bitsandbytes)\"]:\n",
        "        model.to(device)\n",
        "    MODEL_CACHE[cache_key]={\"model\":model,\"tokenizer\":tokenizer}\n",
        "    return model, tokenizer\n",
        "\n",
        "# -------------- BFS => background --------------\n",
        "def run_bfs_in_background(model, tokenizer, model_label, quant, seq_len, attn_comp, lin_comp, def_comp):\n",
        "    global BFS_CANCEL, BFS_RUNNING, BFS_FAIL\n",
        "    global HYP_GRAPH, COMP_GRAPH, OLD_HYP_GRAPH, OLD_COMP_GRAPH\n",
        "    BFS_CANCEL= False\n",
        "    BFS_RUNNING= True\n",
        "    BFS_FAIL= False\n",
        "    log_msg(\"ğŸ”µ BFS background thread => start\")\n",
        "    try:\n",
        "        new_hyp= extract_hypergraph(model)\n",
        "        new_hyp= keep_largest_connected_component(new_hyp)\n",
        "        if BFS_CANCEL:\n",
        "            BFS_RUNNING=False\n",
        "            log_msg(\"ğŸ”´ BFS CANCEL => early exit (hyper).\")\n",
        "            return\n",
        "\n",
        "        new_comp= extract_comp_graph(model, tokenizer)\n",
        "        new_comp= keep_largest_connected_component(new_comp)\n",
        "        if BFS_CANCEL:\n",
        "            BFS_RUNNING=False\n",
        "            log_msg(\"ğŸ”´ BFS CANCEL => early exit (comp).\")\n",
        "            return\n",
        "\n",
        "        compute_bfs_levels(new_comp)\n",
        "        if BFS_CANCEL:\n",
        "            BFS_RUNNING=False\n",
        "            log_msg(\"ğŸ”´ BFS CANCEL => early exit (bfs_levels).\")\n",
        "            return\n",
        "\n",
        "        mark_graph_changes(new_hyp, OLD_HYP_GRAPH)\n",
        "        mark_graph_changes(new_comp, OLD_COMP_GRAPH)\n",
        "\n",
        "        # highlight outdegree>1 => orange\n",
        "        out_degs= new_comp.out_degree()\n",
        "        for nd, deg in out_degs:\n",
        "            if deg>1:\n",
        "                new_comp.nodes[nd][\"color\"]=\"orange\"\n",
        "\n",
        "        from copy import deepcopy\n",
        "        HYP_GRAPH= new_hyp\n",
        "        COMP_GRAPH= new_comp\n",
        "        OLD_HYP_GRAPH= deepcopy(new_hyp)\n",
        "        OLD_COMP_GRAPH= deepcopy(new_comp)\n",
        "\n",
        "        scale_factor= GPU_MEMORY_SCALE.get(\"T4\",1.0)\n",
        "        new_comp.graph[\"seq_len\"]= seq_len\n",
        "        new_comp.graph[\"attn_comp\"]= attn_comp\n",
        "        new_comp.graph[\"lin_comp\"]= lin_comp\n",
        "        new_comp.graph[\"def_comp\"]= def_comp\n",
        "        new_comp.graph[\"scale_factor\"]= scale_factor\n",
        "\n",
        "        for n,d2 in new_comp.nodes(data=True):\n",
        "            if BFS_CANCEL:\n",
        "                BFS_RUNNING=False\n",
        "                log_msg(\"ğŸ”´ BFS CANCEL => early exit (fill mem/time).\")\n",
        "                return\n",
        "            BFS_d= d2.get(\"BFS_distance\",0)\n",
        "            fallback_elems= random.randint(1,10)*1000\n",
        "            shape_mem= (fallback_elems*4)/1024\n",
        "            shape_mem*= random.uniform(0.8,1.2)\n",
        "            base_t= interpret_time_cost(n, attn_comp, lin_comp, def_comp, seq_len)\n",
        "            base_f= compute_node_flops(n, attn_comp, lin_comp, def_comp, seq_len)\n",
        "            mem_init= (BFS_d+1)* shape_mem* scale_factor\n",
        "            time_init= (BFS_d+1)* base_t\n",
        "            flop_val= (BFS_d+1)* base_f\n",
        "\n",
        "            d2[\"mem\"]= mem_init\n",
        "            d2[\"target_mem\"]= mem_init\n",
        "            d2[\"timecost\"]= time_init\n",
        "            d2[\"target_time\"]= time_init*1.5\n",
        "            d2[\"flops\"]= flop_val\n",
        "\n",
        "        log_msg(\"ğŸ”µ BFS background thread => complete\")\n",
        "    except Exception as e:\n",
        "        BFS_FAIL= True\n",
        "        err_msg= f\"âŒ BFS exception => {e}\\n{traceback.format_exc()}\"\n",
        "        log_msg(err_msg)\n",
        "    BFS_RUNNING=False\n",
        "\n",
        "# -------------- MAIN UI + Auto-Reload --------------\n",
        "@app.route(\"/\", methods=[\"GET\",\"POST\"])\n",
        "def main_page():\n",
        "    global BFS_CANCEL, BFS_RUNNING, BFS_FAIL, BFS_THREAD\n",
        "    global RL_CHOICE, CURRENT_MODEL_LABEL, CURRENT_QUANT, CURRENT_REASONING, CURRENT_RL\n",
        "    global COMP_GRAPH,HYP_GRAPH\n",
        "\n",
        "    message_html=\"\"\n",
        "    # A small JS script to auto-reload every 2s if BFS_RUNNING\n",
        "    auto_refresh_js = \"\"\n",
        "    if BFS_RUNNING:\n",
        "        # auto-refresh every 2 seconds\n",
        "        auto_refresh_js = \"\"\"\n",
        "        <script>\n",
        "        setTimeout(() => { window.location.reload(); }, 2000);\n",
        "        </script>\n",
        "        \"\"\"\n",
        "\n",
        "    if request.method==\"POST\":\n",
        "        act= request.form.get(\"act\",\"\")\n",
        "        if act==\"cancel_bfs\":\n",
        "            if BFS_RUNNING:\n",
        "                BFS_CANCEL= True\n",
        "                message_html= \"<p style='color:red;'>Requested BFS cancel. Checking background thread...</p>\"\n",
        "            else:\n",
        "                message_html= \"<p>No BFS is currently running to cancel.</p>\"\n",
        "        else:\n",
        "            # user started BFS\n",
        "            model_label= request.form.get(\"model\",\"BERT (base)\")\n",
        "            hf_id= AVAILABLE_MODELS.get(model_label,\"bert-base-uncased\")\n",
        "            quant= request.form.get(\"quant\",\"None (FP32)\")\n",
        "            reasoning= request.form.get(\"reason\",\"No reasoning\")\n",
        "            rl_algo= request.form.get(\"rl_algo\",\"No RL\")\n",
        "            RL_CHOICE= rl_algo\n",
        "            CURRENT_MODEL_LABEL= model_label\n",
        "            CURRENT_QUANT= quant\n",
        "            CURRENT_REASONING= reasoning\n",
        "            CURRENT_RL= rl_algo\n",
        "\n",
        "            seq_len_str= request.form.get(\"seq_len\",\"32\").strip()\n",
        "            attn_comp_choice= request.form.get(\"attn_comp\",\"Quadratic (n^2)\")\n",
        "            try:\n",
        "                seq_len= int(seq_len_str)\n",
        "                if seq_len<1: seq_len=1\n",
        "            except:\n",
        "                seq_len=32\n",
        "\n",
        "            try:\n",
        "                BFS_CANCEL=False\n",
        "                BFS_RUNNING=True\n",
        "                BFS_FAIL=False\n",
        "                model, tokenizer= load_model(hf_id, model_label, quant)\n",
        "                if rl_algo!=\"No RL\":\n",
        "                    s= random.randint(0,9)\n",
        "                    a= random.randint(0,3)\n",
        "                    r= random.uniform(-1,1)\n",
        "                    update_q_table(s,a,r)\n",
        "\n",
        "                BFS_THREAD= threading.Thread(\n",
        "                    target=run_bfs_in_background,\n",
        "                    daemon=True,\n",
        "                    args=(model, tokenizer, model_label, quant, seq_len, attn_comp_choice, FIXED_LIN_COMP, FIXED_DEF_COMP)\n",
        "                )\n",
        "                BFS_THREAD.start()\n",
        "                message_html= f\"<p><b>Spawned BFS thread => {model_label}, quant={quant}, RL={rl_algo}.</b></p>\"\n",
        "            except Exception as e:\n",
        "                BFS_RUNNING=False\n",
        "                BFS_FAIL=True\n",
        "                message_html= f\"<p style='color:red;'>Model/BFS init failed => {e}</p>\"\n",
        "\n",
        "    if BFS_RUNNING:\n",
        "        message_html+= \"<p style='color:green;'>BFS is running in background. You can cancel below. (Auto-refresh ON)</p>\"\n",
        "    else:\n",
        "        message_html+= \"<p>BFS is not currently running.</p>\"\n",
        "    if BFS_FAIL:\n",
        "        message_html+= \"<p style='color:red;'>BFS FAILED => see debug logs for details. No graphs displayed!</p>\"\n",
        "\n",
        "    model_info_html=\"\"\n",
        "    if CURRENT_MODEL_LABEL or CURRENT_QUANT or CURRENT_RL:\n",
        "        model_info_html= f\"\"\"\n",
        "        <p style=\"color:blue;\">\n",
        "          <b>Currently analyzing:</b>\n",
        "          Model={CURRENT_MODEL_LABEL or '-'},\n",
        "          Quant={CURRENT_QUANT or '-'},\n",
        "          Reasoning={CURRENT_REASONING or '-'},\n",
        "          RL={CURRENT_RL or '-'}\n",
        "        </p>\n",
        "        \"\"\"\n",
        "\n",
        "    model_opts= \"\\n\".join([f'<option>{m}</option>' for m in AVAILABLE_MODELS])\n",
        "    quant_opts= \"\\n\".join([f'<option>{q}</option>' for q in QUANT_METHODS])\n",
        "    reasoning_opts= \"\\n\".join([f'<option>{r}</option>' for r in REASONING_METHODS])\n",
        "    rl_opts= \"\\n\".join([f'<option>{ra}</option>' for ra in RL_ALGORITHMS])\n",
        "    attn_opts= \"\\n\".join([f'<option>{ac}</option>' for ac in ATTN_COMPLEXITY_OPTS])\n",
        "\n",
        "    if BFS_RUNNING or BFS_FAIL or (not COMP_GRAPH):\n",
        "        graph_html= \"<p>No BFS graph => run BFS or wait for BFS completion (or BFS failed/tracing error).</p>\"\n",
        "        qspace_html= \"\"\n",
        "        metrics_html= \"\"\n",
        "    else:\n",
        "        graph_html= show_side_by_side_graphs()\n",
        "        if CURRENT_RL!=\"No RL\":\n",
        "            qspace_html= build_qspace_iframe()\n",
        "        else:\n",
        "            qspace_html=\"\"\n",
        "        if COMP_GRAPH.number_of_nodes()>0:\n",
        "            metrics_html= build_metrics_iframe()\n",
        "        else:\n",
        "            metrics_html=\"\"\n",
        "\n",
        "    html= f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "      <title>AI Interpretability Tool</title>\n",
        "      {auto_refresh_js}\n",
        "    </head>\n",
        "    <body style=\"background-color:#E5D3F2;text-align:center;\">\n",
        "      <h1>AI Interpretability Tool</h1>\n",
        "      <p><a href=\"/debug_logs\" target=\"_blank\">View debug logs</a></p>\n",
        "\n",
        "      <form method=\"POST\">\n",
        "        <h3>Start BFS Analysis</h3>\n",
        "        Model:\n",
        "        <select name=\"model\">{model_opts}</select>\n",
        "        Quant:\n",
        "        <select name=\"quant\">{quant_opts}</select>\n",
        "        Reasoning:\n",
        "        <select name=\"reason\">{reasoning_opts}</select>\n",
        "        RL Algo:\n",
        "        <select name=\"rl_algo\">{rl_opts}</select>\n",
        "        <br><br>\n",
        "        Sequence length: <input name=\"seq_len\" value=\"32\" size=4>\n",
        "        <br>\n",
        "        Attn complexity:\n",
        "        <select name=\"attn_comp\">{attn_opts}</select>\n",
        "        <br><br>\n",
        "        <input type=\"hidden\" name=\"act\" value=\"run\"/>\n",
        "        <input type=\"submit\" value=\"Run BFS\"/>\n",
        "      </form>\n",
        "\n",
        "      <form method=\"POST\" style=\"margin-top:20px;\">\n",
        "        <input type=\"hidden\" name=\"act\" value=\"cancel_bfs\"/>\n",
        "        <input type=\"submit\" value=\"Cancel BFS\"/>\n",
        "      </form>\n",
        "\n",
        "      <hr>\n",
        "      {message_html}\n",
        "      {model_info_html}\n",
        "      <hr>\n",
        "      {graph_html}\n",
        "      {qspace_html}\n",
        "      {metrics_html}\n",
        "\n",
        "      <hr>\n",
        "      <p>\n",
        "        <a href=\"/bottleneck_analyze\">Bottleneck Analysis</a> |\n",
        "        <a href=\"/plot_distributions\" target=\"_blank\">Distribution Plots</a> |\n",
        "        <a href=\"/optimize_model\" target=\"_blank\">Optimize Model</a>\n",
        "      </p>\n",
        "    </body></html>\n",
        "    \"\"\"\n",
        "    return html\n",
        "\n",
        "# -------------- BOTTLENECK --------------\n",
        "@app.route(\"/bottleneck_analyze\", methods=[\"GET\",\"POST\"])\n",
        "def bottleneck_analyze():\n",
        "    global COMP_GRAPH\n",
        "    if not COMP_GRAPH:\n",
        "        return \"<p>No BFS => run BFS first.</p>\"\n",
        "    if request.method==\"POST\":\n",
        "        std_threshold_str= request.form.get(\"std_threshold\",\"2\").strip()\n",
        "        opt_choice= request.form.get(\"opt_choice\",\"pruning\")\n",
        "        try:\n",
        "            std_threshold= float(std_threshold_str)\n",
        "        except:\n",
        "            std_threshold=2.0\n",
        "\n",
        "        time_vals= [d[\"timecost\"] for _, d in COMP_GRAPH.nodes(data=True)]\n",
        "        mem_vals=  [d[\"mem\"] for _, d in COMP_GRAPH.nodes(data=True)]\n",
        "        flop_vals= [d[\"flops\"] for _, d in COMP_GRAPH.nodes(data=True)]\n",
        "        def mean_std(vals):\n",
        "            if not vals: return (0,0)\n",
        "            m= sum(vals)/len(vals)\n",
        "            var= sum((v-m)**2 for v in vals)/ len(vals)\n",
        "            return (m, var**0.5)\n",
        "\n",
        "        t_mean,t_std= mean_std(time_vals)\n",
        "        m_mean,m_std= mean_std(mem_vals)\n",
        "        f_mean,f_std= mean_std(flop_vals)\n",
        "\n",
        "        bottleneck_nodes=[]\n",
        "        for n,d in COMP_GRAPH.nodes(data=True):\n",
        "            tv= d[\"timecost\"]\n",
        "            mv= d[\"mem\"]\n",
        "            fv= d[\"flops\"]\n",
        "            time_exceed= (tv> t_mean+ std_threshold*t_std) if t_std>0 else False\n",
        "            mem_exceed= (mv> m_mean+ std_threshold*m_std) if m_std>0 else False\n",
        "            flop_exceed= (fv> f_mean+ std_threshold*f_std) if f_std>0 else False\n",
        "            if time_exceed or mem_exceed or flop_exceed:\n",
        "                d[\"color\"]=\"magenta\"\n",
        "                bottleneck_nodes.append(n)\n",
        "\n",
        "        if opt_choice.lower()!=\"none\":\n",
        "            for bn in bottleneck_nodes:\n",
        "                nd= COMP_GRAPH.nodes[bn]\n",
        "                nd[\"timecost\"]*=0.7\n",
        "                nd[\"mem\"]*=0.7\n",
        "                nd[\"flops\"]*=0.7\n",
        "\n",
        "        return f\"<h3>Bottleneck Analysis</h3><p>Bottleneck nodes => {bottleneck_nodes}, threshold={std_threshold}, method={opt_choice}</p><p><a href='/'>Back</a></p>\"\n",
        "\n",
        "    return \"\"\"\n",
        "    <h2>Bottleneck Analysis</h2>\n",
        "    <form method=\"POST\">\n",
        "      Stdev threshold: <input name=\"std_threshold\" value=\"2\"/><br><br>\n",
        "      Optimization method:\n",
        "      <select name=\"opt_choice\">\n",
        "        <option>pruning</option>\n",
        "        <option>quantization</option>\n",
        "        <option>tensor-fusion</option>\n",
        "        <option>none</option>\n",
        "      </select><br><br>\n",
        "      <input type='submit' value='Analyze'/>\n",
        "    </form>\n",
        "    <p><a href='/'>Back</a></p>\n",
        "    \"\"\"\n",
        "\n",
        "# -------------- DISTRIBUTIONS --------------\n",
        "@app.route(\"/plot_distributions\")\n",
        "def plot_distributions():\n",
        "    global COMP_GRAPH\n",
        "    if not COMP_GRAPH or COMP_GRAPH.number_of_nodes()==0:\n",
        "        return \"<p>No BFS => run BFS first.</p>\"\n",
        "    time_vals= [d[\"timecost\"] for _,d in COMP_GRAPH.nodes(data=True)]\n",
        "    mem_vals=  [d[\"mem\"] for _,d in COMP_GRAPH.nodes(data=True)]\n",
        "    flop_vals= [d[\"flops\"] for _,d in COMP_GRAPH.nodes(data=True)]\n",
        "    import plotly.subplots as sp\n",
        "    import plotly.graph_objs as go\n",
        "    fig= sp.make_subplots(rows=1,cols=3, subplot_titles=(\"Time Dist\",\"Memory Dist\",\"FLOPs Dist\"))\n",
        "    fig.add_trace(go.Histogram(x=time_vals), row=1,col=1)\n",
        "    fig.add_trace(go.Histogram(x=mem_vals),  row=1,col=2)\n",
        "    fig.add_trace(go.Histogram(x=flop_vals), row=1,col=3)\n",
        "    fig.update_layout(title_text=\"Distribution of Time/Memory/FLOPs\", showlegend=False)\n",
        "    dist_fname=\"static/dist_plots.html\"\n",
        "    fig.write_html(dist_fname, include_plotlyjs='cdn')\n",
        "    return f\"\"\"\n",
        "    <h2>Distribution Plots</h2>\n",
        "    <iframe src=\"/{dist_fname}\" style=\"width:100%; height:600px; border:none;\"></iframe>\n",
        "    <p><a href=\"/\">Back</a></p>\n",
        "    \"\"\"\n",
        "\n",
        "# -------------- EXPORT BFS / Q --------------\n",
        "@app.route(\"/export_data\")\n",
        "def export_data():\n",
        "    global COMP_GRAPH,HYP_GRAPH\n",
        "    if not COMP_GRAPH:\n",
        "        return \"No BFS => run BFS first.\"\n",
        "    graph_choice= request.args.get(\"graph\",\"comp\").lower()\n",
        "    fmt= request.args.get(\"fmt\",\"csv\").lower()\n",
        "    nxg= HYP_GRAPH if graph_choice==\"hyp\" else COMP_GRAPH\n",
        "    rows=[]\n",
        "    for n,d in nxg.nodes(data=True):\n",
        "        row={\n",
        "            \"node_id\":n,\n",
        "            \"BFS_distance\":d.get(\"BFS_distance\",0),\n",
        "            \"mem\":d.get(\"mem\",0),\n",
        "            \"timecost\":d.get(\"timecost\",0),\n",
        "            \"flops\":d.get(\"flops\",0),\n",
        "            \"color\":d.get(\"color\",\"\"),\n",
        "        }\n",
        "        rows.append(row)\n",
        "    if fmt==\"json\":\n",
        "        return jsonify(rows)\n",
        "    else:\n",
        "        df= pd.DataFrame(rows)\n",
        "        csv_data= df.to_csv(index=False)\n",
        "        return Response(\n",
        "            csv_data,\n",
        "            mimetype=\"text/csv\",\n",
        "            headers={\"Content-disposition\":\"attachment; filename=bfs_data.csv\"}\n",
        "        )\n",
        "\n",
        "@app.route(\"/export_qspace\")\n",
        "def export_qspace():\n",
        "    rows=[]\n",
        "    for s in range(10):\n",
        "        for a in range(4):\n",
        "            val= Q_TABLE[s][a]\n",
        "            rows.append({\"state\":s,\"action\":a,\"q_value\":val})\n",
        "    fmt= request.args.get(\"fmt\",\"csv\").lower()\n",
        "    if fmt==\"json\":\n",
        "        return jsonify(rows)\n",
        "    else:\n",
        "        import pandas as pd\n",
        "        df= pd.DataFrame(rows)\n",
        "        csv_data= df.to_csv(index=False)\n",
        "        return Response(\n",
        "            csv_data,\n",
        "            mimetype=\"text/csv\",\n",
        "            headers={\"Content-disposition\":\"attachment; filename=qspace_data.csv\"}\n",
        "        )\n",
        "\n",
        "# -------------- Show side-by-side BFS => hyper / comp --------------\n",
        "from pyvis.network import Network\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def show_side_by_side_graphs():\n",
        "    global HYP_GRAPH, COMP_GRAPH, BFS_FAIL\n",
        "    if BFS_FAIL:\n",
        "        return \"<p style='color:red;'>BFS failed => no graphs to show.</p>\"\n",
        "    if not HYP_GRAPH or not COMP_GRAPH:\n",
        "        return \"<p>No BFS graphs => run BFS or BFS not done or BFS failed/tracing error.</p>\"\n",
        "\n",
        "    net_h= Network(height=\"600px\",width=\"100%\",directed=False)\n",
        "    net_h.barnes_hut()\n",
        "    net_h.set_options(\"\"\"\n",
        "    var options={\n",
        "      \"physics\": {\n",
        "        \"enabled\":true,\n",
        "        \"stabilization\":{\"iterations\":500}\n",
        "      }\n",
        "    }\"\"\")\n",
        "    for n,d in HYP_GRAPH.nodes(data=True):\n",
        "        clr= d.get(\"color\",\"blue\")\n",
        "        net_h.add_node(n,label=n,color=clr)\n",
        "    for u,v in HYP_GRAPH.edges():\n",
        "        net_h.add_edge(u,v,color=\"gray\")\n",
        "    hyper_fname=\"static/hypergraph_full.html\"\n",
        "    net_h.write_html(hyper_fname, notebook=False, open_browser=False)\n",
        "\n",
        "    net_c= Network(height=\"600px\",width=\"100%\",directed=False)\n",
        "    net_c.barnes_hut()\n",
        "    net_c.set_options(\"\"\"\n",
        "    var options={\n",
        "      \"physics\": {\n",
        "        \"enabled\":true,\n",
        "        \"stabilization\":{\"iterations\":500}\n",
        "      }\n",
        "    }\"\"\")\n",
        "    for n,d in COMP_GRAPH.nodes(data=True):\n",
        "        clr= d.get(\"color\",\"blue\")\n",
        "        BFS_d= d.get(\"BFS_distance\",0)\n",
        "        memv= d.get(\"mem\",0)\n",
        "        timv= d.get(\"timecost\",0)\n",
        "        flps= d.get(\"flops\",0)\n",
        "        title_txt= f\"BFS={BFS_d}, mem={memv:.2f}, time={timv:.2f}, flops={flps}\"\n",
        "        net_c.add_node(n,label=n,color=clr,title=title_txt)\n",
        "    for u,v in COMP_GRAPH.edges():\n",
        "        clr= COMP_GRAPH[u][v].get(\"color\",\"gray\")\n",
        "        net_c.add_edge(u,v,color=clr)\n",
        "    comp_fname=\"static/compgraph_full.html\"\n",
        "    net_c.write_html(comp_fname, notebook=False, open_browser=False)\n",
        "\n",
        "    return f\"\"\"\n",
        "    <div style=\"display:flex; flex-direction:row; gap:20px; justify-content:space-evenly; margin-top:10px;\">\n",
        "      <div style=\"flex:1;\">\n",
        "        <h3 style=\"text-align:center; font-size:2em;\">Hypergraph</h3>\n",
        "        <iframe src=\"/{hyper_fname}\" style=\"width:100%; height:600px; border:none;\"></iframe>\n",
        "        <p style=\"text-align:center;\">\n",
        "          <a href=\"/export_data?graph=hyp&fmt=csv\" target=\"_blank\">CSV</a> |\n",
        "          <a href=\"/export_data?graph=hyp&fmt=json\" target=\"_blank\">JSON</a>\n",
        "        </p>\n",
        "      </div>\n",
        "      <div style=\"flex:1;\">\n",
        "        <h3 style=\"text-align:center; font-size:2em;\">Computational Graph</h3>\n",
        "        <iframe src=\"/{comp_fname}\" style=\"width:100%; height:600px; border:none;\"></iframe>\n",
        "        <p style=\"text-align:center;\">\n",
        "          <a href=\"/export_data?graph=comp&fmt=csv\" target=\"_blank\">CSV</a> |\n",
        "          <a href=\"/export_data?graph=comp&fmt=json\" target=\"_blank\">JSON</a>\n",
        "        </p>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def build_qspace_iframe():\n",
        "    xs,ys,zs=[],[],[]\n",
        "    for s in range(10):\n",
        "        for a in range(4):\n",
        "            xs.append(s)\n",
        "            ys.append(a)\n",
        "            zs.append(Q_TABLE[s][a])\n",
        "    fig= go.Figure(data=[go.Scatter3d(\n",
        "        x=xs,y=ys,z=zs,\n",
        "        mode='markers',\n",
        "        marker=dict(size=5,color=zs,colorscale='Viridis'),\n",
        "        text=[f\"Q({s},{a})={val:.2f}\" for s,a,val in zip(xs,ys,zs)]\n",
        "    )])\n",
        "    fig.update_layout(\n",
        "        title={'text':\"Q(s,a) 3D space\",'x':0.5,'xanchor':'center','font':{'size':24}},\n",
        "        scene=dict(\n",
        "            xaxis_title='State(s)',\n",
        "            yaxis_title='Action(a)',\n",
        "            zaxis_title='Q-value'\n",
        "        )\n",
        "    )\n",
        "    qspace_fname=\"static/qspace.html\"\n",
        "    fig.write_html(qspace_fname, include_plotlyjs='cdn')\n",
        "    return f\"\"\"\n",
        "    <div style=\"text-align:center; margin-top:20px;\">\n",
        "      <h3 style=\"font-size:2em;\">Q-space 3D</h3>\n",
        "      <iframe src=\"/{qspace_fname}\" style=\"width:100%; height:600px; border:none;\"></iframe>\n",
        "      <p>\n",
        "        <a href=\"/export_qspace?fmt=csv\" target=\"_blank\">CSV</a> |\n",
        "        <a href=\"/export_qspace?fmt=json\" target=\"_blank\">JSON</a>\n",
        "      </p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "def build_metrics_iframe():\n",
        "    global COMP_GRAPH\n",
        "    if not COMP_GRAPH or COMP_GRAPH.number_of_nodes()==0:\n",
        "        return \"\"\n",
        "    import plotly.graph_objs as go\n",
        "    mem_vals=[]\n",
        "    time_vals=[]\n",
        "    flop_vals=[]\n",
        "    labels=[]\n",
        "    for n,d in COMP_GRAPH.nodes(data=True):\n",
        "        mem_vals.append(d[\"mem\"])\n",
        "        time_vals.append(d[\"timecost\"])\n",
        "        flop_vals.append(d[\"flops\"])\n",
        "        labels.append(n)\n",
        "    fig= go.Figure(data=[go.Scatter3d(\n",
        "        x=mem_vals,\n",
        "        y=time_vals,\n",
        "        z=flop_vals,\n",
        "        mode='markers',\n",
        "        marker=dict(size=5,color=time_vals,colorscale='Viridis'),\n",
        "        text=labels\n",
        "    )])\n",
        "    fig.update_layout(\n",
        "        title={'text':\"Memory/Time/FLOPs (3D)\",\"x\":0.5,\"xanchor\":\"center\",\"font\":{\"size\":24}},\n",
        "        scene=dict(\n",
        "            xaxis_title='Memory(KB)',\n",
        "            yaxis_title='Time(ms)',\n",
        "            zaxis_title='FLOPs'\n",
        "        )\n",
        "    )\n",
        "    metric_fname=\"static/metrics_3d.html\"\n",
        "    fig.write_html(metric_fname, include_plotlyjs='cdn')\n",
        "    return f\"\"\"\n",
        "    <div style=\"text-align:center; margin-top:20px;\">\n",
        "      <h3 style=\"font-size:2em;\">Memory/Time/FLOPs 3D</h3>\n",
        "      <iframe src=\"/{metric_fname}\" style=\"width:100%; height:600px; border:none;\"></iframe>\n",
        "      <p><a href=\"/export_data?graph=comp&fmt=csv\" target=\"_blank\">CSV</a> |\n",
        "         <a href=\"/export_data?graph=comp&fmt=json\" target=\"_blank\">JSON</a></p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "# -------------- DEBUG LOGS --------------\n",
        "@app.route(\"/debug_logs\")\n",
        "def debug_logs():\n",
        "    lines= \"<br>\".join(LOG_BUFFER)\n",
        "    return f\"<h2>Debug Logs</h2><pre>{lines}</pre>\"\n",
        "\n",
        "# -------------- RUN FLASK --------------\n",
        "def run_flask():\n",
        "    log_msg(f\"[DEBUG] run_flask => listening on port {PORT}\")\n",
        "    app.run(host=\"0.0.0.0\", port=PORT, debug=False)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    flask_thread= threading.Thread(target=run_flask, daemon=True)\n",
        "    flask_thread.start()\n",
        "    time.sleep(3)\n",
        "    if SCENARIO==\"domain\":\n",
        "        log_msg(\"ğŸŒ SCENARIO=domain => check http://127.0.0.1:8000\")\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    elif SCENARIO==\"ngrok\":\n",
        "        log_msg(\"ğŸŒ SCENARIO=ngrok => ephemeral link => default.\")\n",
        "        try:\n",
        "            from pyngrok import ngrok\n",
        "            ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "            public_url= ngrok.connect(PORT)\n",
        "            log_msg(f\"ğŸŒ Public URL => {public_url.public_url}\")\n",
        "            log_msg(\"âœ… Keep script running => Ctrl+C to stop.\")\n",
        "            while True:\n",
        "                time.sleep(1)\n",
        "        except Exception as e:\n",
        "            log_msg(f\"âš ï¸ Could not start ngrok => {e}\")\n",
        "            while True:\n",
        "                time.sleep(1)\n",
        "    elif SCENARIO==\"cloudflared\":\n",
        "        log_msg(\"ğŸŒ SCENARIO=cloudflared => ephemeral link.\")\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    else:\n",
        "        log_msg(f\"â“ Unknown SCENARIO={SCENARIO}, exiting...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "YZx2zi6lfX0p",
        "outputId": "9d0a25e8-df6a-4775-83d7-57a1407c38a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-75774a5ac569>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   5202\u001b[0m \u001b[0mregister_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardsigmoid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardsigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5203\u001b[0m \u001b[0mregister_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iand__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__and__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5204\u001b[0;31m \u001b[0mregister_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ilshift__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__lshift__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5205\u001b[0m \u001b[0mregister_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_put_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_put\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5206\u001b[0m \u001b[0mregister_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_reduce_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_reduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m             \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_op_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 raise AttributeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m_get_packet\u001b[0;34m(qualname, op_module)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m     \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_get_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;31m# let the script frontend know that op is identical to the builtin op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
